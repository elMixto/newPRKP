{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "from src.data_structures import Instance\n",
    "from primes import primes\n",
    "import seaborn as sns\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "instance = Instance.generate(128,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Primero explorar que tan factible es resolver esto para las polys de 2 dimensiones\n",
    "polys = torch.zeros((instance.n_items,instance.n_items))\n",
    "\n",
    "#Hago la matriz simetrica :D\n",
    "treshold = 0\n",
    "func = np.random.random\n",
    "for i in range(len(polys)):\n",
    "    for j in range(len(polys)):\n",
    "        if np.random.random() > treshold:\n",
    "            polys[i,j] = func()\n",
    "            #polys[j,i] = polys[j,i]\n",
    "            if i == j:\n",
    "                polys[i,j] = 0\n",
    "\n",
    "sns.heatmap(polys)\n",
    "plt.legend()\n",
    "plt.savefig(\"real.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "powers_of_two = np.array([ primes[i] for i in range(0,batch_size)])\n",
    "#Genero los inputs\n",
    "inputs = list(combinations(range(instance.n_items),2))\n",
    "def int_to_vec(tupla):\n",
    "    output = np.zeros(instance.n_items)\n",
    "    for x in tupla:\n",
    "        output[x]= 1\n",
    "    return output\n",
    "\n",
    "values = []\n",
    "inputs2 = []\n",
    "for tupla in inputs:\n",
    "    if polys[tupla[0],tupla[1]] != 0:\n",
    "        values.append(polys[tupla[0],tupla[1]])\n",
    "        inputs2.append(tupla)\n",
    "\n",
    "tuplas_vects = list(map(int_to_vec,inputs2))\n",
    "\n",
    "\n",
    "tuplas = inputs2.copy()\n",
    "inputs = np.array(list(map(int_to_vec,inputs2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_nums(test_vec):\n",
    "    test_vec = np.array_split(test_vec,instance.n_items//batch_size)\n",
    "    test_vec = np.array([np.multiply(chunk,powers_of_two) for chunk in test_vec])\n",
    "    test_vec = np.array(list(map(lambda x: np.sum(x),test_vec)))\n",
    "    return test_vec\n",
    "\n",
    "tuplas_vects = np.array(list(map(vec_to_nums,tuplas_vects)))\n",
    "inputs = tuplas_vects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = instance.n_items//batch_size\n",
    "salidas = 1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define the layers of your neural network\n",
    "        hidden = 100\n",
    "        self.fc1 = nn.Linear(entradas, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, salidas)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.output_activation(self.fc1(x)) \n",
    "        x = self.output_activation(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.L1Loss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,0.1,5e-3,10000)\n",
    "train_in = inputs\n",
    "expected_out = tensor(values)\n",
    "epoch = 0\n",
    "train_tensor = torch.tensor(train_in)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando :D 1 0.11090992006235012 0.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando :D 1 0.3361326920492899 0.11111"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    with torch.no_grad():\n",
    "        salida_predicha = net(train_tensor)\n",
    "\n",
    "    for i,value in enumerate(salida_predicha):\n",
    "        a = salida_predicha[i][0]\n",
    "        b = expected_out[i]\n",
    "        a.requires_grad = True\n",
    "        b.requires_grad = True\n",
    "        loss = criterion(a,b)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #scheduler.step()\n",
    "        sys.stdout.write(f\"\\rEntrenando :D {epoch+1} {loss.item()} {optimizer.param_groups[0]['lr']}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        #a = net(train_tensor).T\n",
    "        #predicted = np.zeros((instance.n_items,instance.n_items))\n",
    "        #for i,tupla in enumerate(tuplas):\n",
    "        #    predicted[tupla[0],tupla[1]] = a[0][i]\n",
    "        #sns.heatmap(predicted)\n",
    "        #plt.savefig(\"predicted.png\")\n",
    "        #plt.clf()\n",
    "        #sns.heatmap(np.abs(polys-predicted))\n",
    "        #plt.savefig(\"difference.png\")\n",
    "        #plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = net(tensor(train_in)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "predicted = np.zeros((instance.n_items,instance.n_items))\n",
    "for i,tupla in enumerate(tuplas):\n",
    "    predicted[tupla[0],tupla[1]] = a[0][i]\n",
    "    #predicted[tupla[1],tupla[0]] = a[0][i]\n",
    "\n",
    "#plt.plot(test_data, a[0].detach().numpy(), label='Preds')\n",
    "\n",
    "sns.heatmap(polys)\n",
    "plt.title(loss.item())\n",
    "plt.legend()\n",
    "plt.savefig(\"real.png\")\n",
    "plt.clf()\n",
    "plt.title(loss.item())\n",
    "sns.heatmap(predicted)\n",
    "plt.savefig(\"predicted.png\")\n",
    "plt.clf()\n",
    "plt.title(loss.item())\n",
    "sns.heatmap(\n",
    "    np.abs(polys-predicted)\n",
    "    )\n",
    "plt.savefig(\"difference.png\")\n",
    "plt.clf()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
