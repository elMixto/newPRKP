{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "from src.data_structures import Instance\n",
    "import seaborn as sns\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464\n"
     ]
    }
   ],
   "source": [
    "instance = Instance.generate(256,1)\n",
    "print(len(instance.polynomial_gains.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Primero explorar que tan factible es resolver esto para las polys de 2 dimensiones\n",
    "polys = torch.zeros((instance.n_items,instance.n_items))\n",
    "\n",
    "#Hago la matriz simetrica :D\n",
    "treshold = 0.8\n",
    "func = np.random.random\n",
    "for i in range(len(polys)):\n",
    "    for j in range(len(polys)):\n",
    "        if np.random.random() > treshold:\n",
    "            polys[i,j] = func()\n",
    "            polys[j,i] = polys[j,i]\n",
    "            if i == j:\n",
    "                polys[i,j] = 0\n",
    "\n",
    "sns.heatmap(polys)\n",
    "plt.legend()\n",
    "plt.savefig(\"real.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "#Genero los inputs\n",
    "inputs = list(combinations(range(instance.n_items),2))\n",
    "def int_to_vec(tupla):\n",
    "    output = np.zeros(instance.n_items)\n",
    "    for x in tupla:\n",
    "        output[x]= 1\n",
    "    return output\n",
    "values = []\n",
    "inputs2 = []\n",
    "for tupla in inputs:\n",
    "    if polys[tupla[0],tupla[1]] != 0:\n",
    "        values.append(polys[tupla[0],tupla[1]])\n",
    "        inputs2.append(tupla)\n",
    "\n",
    "values = tensor(values)\n",
    "tuplas = inputs2.copy()\n",
    "inputs = np.array(list(map(int_to_vec,inputs2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = instance.n_items\n",
    "salidas = 1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden = 10\n",
    "        # Define the layers of your neural network\n",
    "        self.fc1 = nn.Linear(entradas, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, salidas)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.output_activation(self.fc1(x))\n",
    "        x = self.output_activation(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.L1Loss()\n",
    "learning_rate = 1\n",
    "optimizer = torch.optim.ASGD(net.parameters(), lr=learning_rate)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer,0.1,5e-3,10000)\n",
    "train_in = inputs\n",
    "expected_out = values\n",
    "epoch = 0\n",
    "train_tensor = torch.tensor(train_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mixto/repositories/PRKP/from_scratch.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mixto/repositories/PRKP/from_scratch.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mixto/repositories/PRKP/from_scratch.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mixto/repositories/PRKP/from_scratch.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mixto/repositories/PRKP/from_scratch.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m#scheduler.step()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mixto/repositories/PRKP/from_scratch.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39mEntrenando :D \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/asgd.py:120\u001b[0m, in \u001b[0;36mASGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m     state_steps \u001b[39m=\u001b[39m []\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, grads, mus, axs, etas, state_steps)\n\u001b[0;32m--> 120\u001b[0m     asgd(\n\u001b[1;32m    121\u001b[0m         params_with_grad,\n\u001b[1;32m    122\u001b[0m         grads,\n\u001b[1;32m    123\u001b[0m         axs,\n\u001b[1;32m    124\u001b[0m         mus,\n\u001b[1;32m    125\u001b[0m         etas,\n\u001b[1;32m    126\u001b[0m         state_steps,\n\u001b[1;32m    127\u001b[0m         lambd\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlambd\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    128\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    129\u001b[0m         t0\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mt0\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    130\u001b[0m         alpha\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39malpha\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    131\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    132\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    133\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    134\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    137\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/asgd.py:198\u001b[0m, in \u001b[0;36masgd\u001b[0;34m(params, grads, axs, mus, etas, state_steps, foreach, maximize, differentiable, lambd, lr, t0, alpha, weight_decay)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_asgd\n\u001b[0;32m--> 198\u001b[0m func(\n\u001b[1;32m    199\u001b[0m     params,\n\u001b[1;32m    200\u001b[0m     grads,\n\u001b[1;32m    201\u001b[0m     axs,\n\u001b[1;32m    202\u001b[0m     mus,\n\u001b[1;32m    203\u001b[0m     etas,\n\u001b[1;32m    204\u001b[0m     state_steps,\n\u001b[1;32m    205\u001b[0m     lambd\u001b[39m=\u001b[39;49mlambd,\n\u001b[1;32m    206\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    207\u001b[0m     t0\u001b[39m=\u001b[39;49mt0,\n\u001b[1;32m    208\u001b[0m     alpha\u001b[39m=\u001b[39;49malpha,\n\u001b[1;32m    209\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    210\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    211\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    212\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/asgd.py:269\u001b[0m, in \u001b[0;36m_single_tensor_asgd\u001b[0;34m(params, grads, axs, mus, etas, state_steps, lambd, lr, t0, alpha, weight_decay, maximize, differentiable)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     ax\u001b[39m.\u001b[39mcopy_(param)\n\u001b[0;32m--> 269\u001b[0m new_eta \u001b[39m=\u001b[39m _to_tensor(lr \u001b[39m/\u001b[39;49m ((\u001b[39m1\u001b[39;49m \u001b[39m+\u001b[39;49m lambd \u001b[39m*\u001b[39;49m lr \u001b[39m*\u001b[39;49m step) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m alpha))\n\u001b[1;32m    270\u001b[0m eta\u001b[39m.\u001b[39mcopy_(new_eta)\n\u001b[1;32m    271\u001b[0m new_mu \u001b[39m=\u001b[39m _to_tensor(\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m1\u001b[39m, step \u001b[39m-\u001b[39m t0))\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/asgd.py:233\u001b[0m, in \u001b[0;36m_single_tensor_asgd.<locals>._to_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_tensor\u001b[39m(x):\n\u001b[1;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 233\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(x)\n\u001b[1;32m    234\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    for i,value in enumerate(expected_out):\n",
    "        salida_predicha = net(train_tensor[i])[0]\n",
    "        a = salida_predicha\n",
    "        #a.requires_grad = True\n",
    "        #b.requires_grad = True\n",
    "        loss = criterion(a,value)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #scheduler.step()\n",
    "    sys.stdout.write(f\"\\rEntrenando :D {epoch+1} {loss.item()} {optimizer.param_groups[0]['lr']}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "        #a = net(train_tensor).T\n",
    "        #predicted = np.zeros((instance.n_items,instance.n_items))\n",
    "        #for i,tupla in enumerate(tuplas):\n",
    "        #    predicted[tupla[0],tupla[1]] = a[0][i]\n",
    "        #sns.heatmap(predicted)\n",
    "        #plt.savefig(\"predicted.png\")\n",
    "        #plt.clf()\n",
    "        #sns.heatmap(np.abs(polys-predicted))\n",
    "        #plt.savefig(\"difference.png\")\n",
    "        #plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = net(tensor(train_in)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "predicted = np.zeros((instance.n_items,instance.n_items))\n",
    "for i,tupla in enumerate(tuplas):\n",
    "    predicted[tupla[0],tupla[1]] = a[0][i]\n",
    "\n",
    "sns.heatmap(polys)\n",
    "plt.legend()\n",
    "plt.savefig(\"real.png\")\n",
    "plt.clf()\n",
    "sns.heatmap(predicted)\n",
    "plt.savefig(\"predicted.png\")\n",
    "plt.clf()\n",
    "plt.title(loss.item())\n",
    "sns.heatmap(np.abs(polys-predicted))\n",
    "plt.savefig(\"difference.png\")\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6675]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.zeros(instance.n_items)\n",
    "a = net(tensor([test]))\n",
    "a\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
